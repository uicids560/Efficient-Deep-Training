{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uicids560/Efficient-Deep-Training/blob/main/Cifar100_GoogLeNet_Entire_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmJIWgHCw0zw"
      },
      "source": [
        "# GoogLeNet - CIFAR100\n",
        "GoogLeNet Model Architecture from CORDS: https://github.com/decile-team/cords/blob/main/train_sl.py "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plXHZIDJg5yE"
      },
      "source": [
        "## Cloning CORDS repository & Install prerequisite libraries of CORDS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0dB19yvphJVL",
        "outputId": "c85ee672-e72d-4773-9857-7f6f4757cdab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cords'...\n",
            "remote: Enumerating objects: 4718, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 4718 (delta 12), reused 11 (delta 4), pack-reused 4685\u001b[K\n",
            "Receiving objects: 100% (4718/4718), 58.35 MiB | 21.13 MiB/s, done.\n",
            "Resolving deltas: 100% (2863/2863), done.\n",
            "/content/cords\n",
            "\u001b[0m\u001b[01;34mbenchmarks\u001b[0m/   \u001b[01;34mcords\u001b[0m/     LICENSE.txt    setup.py      train_sl.py\n",
            "CITATION.CFF  \u001b[01;34mdocs\u001b[0m/      README.md      \u001b[01;34mtests\u001b[0m/        train_ssl.py\n",
            "\u001b[01;34mconfigs\u001b[0m/      \u001b[01;34mexamples\u001b[0m/  \u001b[01;34mrequirements\u001b[0m/  train_hpo.py\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.30-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dotmap\n",
            "Successfully installed dotmap-1.3.30\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting apricot-select\n",
            "  Downloading apricot-select-0.6.1.tar.gz (28 kB)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (1.7.3)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (0.56.4)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (4.64.1)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 13.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->apricot-select) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->apricot-select) (57.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->apricot-select) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->apricot-select) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->numba>=0.43.0->apricot-select) (4.1.1)\n",
            "Building wheels for collected packages: apricot-select\n",
            "  Building wheel for apricot-select (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apricot-select: filename=apricot_select-0.6.1-py3-none-any.whl size=48785 sha256=6155c5950b2ddc997d505600779e9c5464a92e5d5a2180960daa40b0de0b2764\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b0/5d/41bab30f23d17864700963dad70bbeda159a409e94f0778f2f\n",
            "Successfully built apricot-select\n",
            "Installing collected packages: nose, apricot-select\n",
            "Successfully installed apricot-select-0.6.1 nose-1.3.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ray[default]\n",
            "  Downloading ray-2.1.0-cp37-cp37m-manylinux2014_x86_64.whl (59.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.1 MB 280 kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[default]) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[default]) (2.23.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[default]) (22.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray[default]) (4.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.8.0)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.16.7-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 53.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.21.6)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[default]) (4.3.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.3.3)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.50.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.0.4)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (7.1.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.19.6)\n",
            "Collecting aiohttp-cors\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful\n",
            "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[K     |████████████████████████████████| 201 kB 78.2 MB/s \n",
            "\u001b[?25hCollecting prometheus-client<0.14.0,>=0.7.1\n",
            "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.7/dist-packages (from ray[default]) (3.8.3)\n",
            "Collecting gpustat>=1.0.0\n",
            "  Downloading gpustat-1.0.0.tar.gz (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 11.6 MB/s \n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 68.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from ray[default]) (1.10.2)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.7/dist-packages (from ray[default]) (5.2.1)\n",
            "Collecting opencensus\n",
            "  Downloading opencensus-0.11.0-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[K     |████████████████████████████████| 128 kB 86.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]) (2.1.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[default]) (0.13.0)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0->ray[default]) (1.15.0)\n",
            "Collecting nvidia-ml-py<=11.495.46,>=11.450.129\n",
            "  Downloading nvidia_ml_py-11.495.46-py3-none-any.whl (25 kB)\n",
            "Collecting psutil>=5.6.0\n",
            "  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)\n",
            "\u001b[K     |████████████████████████████████| 280 kB 88.0 MB/s \n",
            "\u001b[?25hCollecting blessed>=1.17.1\n",
            "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0->ray[default]) (0.2.5)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 80.3 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2.4\n",
            "  Downloading platformdirs-2.5.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.24->ray[default]) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.3->virtualenv>=20.0.24->ray[default]) (3.10.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.7->ray[default]) (2.10)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[default]) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[default]) (0.19.2)\n",
            "Collecting opencensus-context>=0.1.3\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[default]) (2.8.2)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (2.14.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (1.56.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (5.2.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.2.8)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default]) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[default]) (3.0.4)\n",
            "Building wheels for collected packages: gpustat\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-1.0.0-py3-none-any.whl size=19887 sha256=6b68d325f3a7d0be18fd86d045f851fc2fbd19301419c07994d366c561c033bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/31/5c/eb69af6e2285e7d6ec8d7dc26435be7c81c6ad22c45efdcca7\n",
            "Successfully built gpustat\n",
            "Installing collected packages: platformdirs, distlib, virtualenv, psutil, opencensus-context, nvidia-ml-py, blessed, ray, py-spy, prometheus-client, opencensus, gpustat, colorful, aiohttp-cors\n",
            "  Attempting uninstall: psutil\n",
            "    Found existing installation: psutil 5.4.8\n",
            "    Uninstalling psutil-5.4.8:\n",
            "      Successfully uninstalled psutil-5.4.8\n",
            "  Attempting uninstall: prometheus-client\n",
            "    Found existing installation: prometheus-client 0.15.0\n",
            "    Uninstalling prometheus-client-0.15.0:\n",
            "      Successfully uninstalled prometheus-client-0.15.0\n",
            "Successfully installed aiohttp-cors-0.7.0 blessed-1.19.1 colorful-0.5.4 distlib-0.3.6 gpustat-1.0.0 nvidia-ml-py-11.495.46 opencensus-0.11.0 opencensus-context-0.1.3 platformdirs-2.5.4 prometheus-client-0.13.1 psutil-5.9.4 py-spy-0.3.14 ray-2.1.0 virtualenv-20.16.7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.7/dist-packages (2.1.0)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (22.1.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (7.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.8.0)\n",
            "Requirement already satisfied: virtualenv>=20.0.24 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (20.16.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.1.1)\n",
            "Requirement already satisfied: grpcio>=1.32.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.50.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.3)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (4.3.3)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (3.19.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (2.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.0.4)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (1.3.5)\n",
            "Collecting tensorboardX>=1.9\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]) (0.8.10)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio>=1.32.0->ray[tune]) (1.15.0)\n",
            "Requirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.24->ray[tune]) (2.5.4)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.3 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.24->ray[tune]) (4.13.0)\n",
            "Requirement already satisfied: distlib<1,>=0.3.6 in /usr/local/lib/python3.7/dist-packages (from virtualenv>=20.0.24->ray[tune]) (0.3.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.3->virtualenv>=20.0.24->ray[tune]) (3.10.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (5.10.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]) (0.19.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[tune]) (3.0.4)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/decile-team/cords.git\n",
        "%cd cords/\n",
        "%ls\n",
        "\n",
        "!pip install dotmap\n",
        "!pip install apricot-select\n",
        "!pip install ray[default]\n",
        "!pip install ray[tune]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAe7hUD2g-_4"
      },
      "source": [
        "##Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksBX97-0hQA8",
        "outputId": "ff46c968-0633-4b00-bee2-4db29c036a23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from cords.utils.data.datasets.SL import gen_dataset\n",
        "from torch.utils.data import Subset\n",
        "from cords.utils.config_utils import load_config_data\n",
        "import os.path as osp\n",
        "from cords.utils.data.data_utils import WeightedSubset\n",
        "from ray import tune\n",
        "#Clean CUDA Memory\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPbYKHh6hcEZ"
      },
      "source": [
        "## Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y4CQQDFqhfpT"
      },
      "outputs": [],
      "source": [
        "from cords.utils.models import GoogLeNet\n",
        "numclasses = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztvfnNNMivdr"
      },
      "outputs": [],
      "source": [
        "from cords.utils.config_utils import load_config_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RX_onTKEcygc"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import os.path as osp\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from ray import tune\n",
        "from torch.utils.data import Subset\n",
        "from cords.utils.config_utils import load_config_data\n",
        "from cords.utils.data.data_utils import WeightedSubset\n",
        "from cords.utils.data.data_utils import collate\n",
        "from cords.utils.data.dataloader.SL.adaptive import GLISTERDataLoader, OLRandomDataLoader, \\\n",
        "    CRAIGDataLoader, GradMatchDataLoader, RandomDataLoader, SELCONDataLoader\n",
        "from cords.utils.data.dataloader.SL.nonadaptive import FacLocDataLoader\n",
        "from cords.utils.data.datasets.SL import gen_dataset\n",
        "from cords.utils.models import *\n",
        "from cords.utils.data.data_utils.collate import *\n",
        "import pickle\n",
        "\n",
        "class TrainClassifier:\n",
        "    def __init__(self, config_file_data):\n",
        "        self.cfg = config_file_data\n",
        "        results_dir = osp.abspath(osp.expanduser(self.cfg.train_args.results_dir))\n",
        "        \n",
        "        if self.cfg.dss_args.type != \"Full\":\n",
        "            all_logs_dir = os.path.join(results_dir, self.cfg.setting,\n",
        "                                        self.cfg.dss_args.type,\n",
        "                                        self.cfg.dataset.name,\n",
        "                                        str(self.cfg.dss_args.fraction),\n",
        "                                        str(self.cfg.dss_args.select_every))\n",
        "        else:\n",
        "            all_logs_dir = os.path.join(results_dir, self.cfg.setting,\n",
        "                                        self.cfg.dss_args.type,\n",
        "                                        self.cfg.dataset.name)\n",
        "\n",
        "        os.makedirs(all_logs_dir, exist_ok=True)\n",
        "        # setup logger\n",
        "        plain_formatter = logging.Formatter(\"[%(asctime)s] %(name)s %(levelname)s: %(message)s\",\n",
        "                                            datefmt=\"%m/%d %H:%M:%S\")\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.logger.setLevel(logging.INFO)\n",
        "        s_handler = logging.StreamHandler(stream=sys.stdout)\n",
        "        s_handler.setFormatter(plain_formatter)\n",
        "        s_handler.setLevel(logging.INFO)\n",
        "        self.logger.addHandler(s_handler)\n",
        "        f_handler = logging.FileHandler(os.path.join(all_logs_dir, self.cfg.dataset.name + \"_\" +\n",
        "                                                     self.cfg.dss_args.type + \".log\"))\n",
        "        f_handler.setFormatter(plain_formatter)\n",
        "        f_handler.setLevel(logging.DEBUG)\n",
        "        self.logger.addHandler(f_handler)\n",
        "        self.logger.propagate = False\n",
        "\n",
        "    \"\"\"\n",
        "    ############################## Loss Evaluation ##############################\n",
        "    \"\"\"\n",
        "\n",
        "    def model_eval_loss(self, data_loader, model, criterion):\n",
        "        total_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "                inputs, targets = inputs.to(self.cfg.train_args.device), \\\n",
        "                                  targets.to(self.cfg.train_args.device, non_blocking=True)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_loss += loss.item()\n",
        "        return total_loss\n",
        "\n",
        "    \"\"\"\n",
        "    ############################## Model Creation ##############################\n",
        "    \"\"\"\n",
        "\n",
        "    def create_model(self):\n",
        "        if self.cfg.model.architecture == 'RegressionNet':\n",
        "            model = RegressionNet(self.cfg.model.input_dim)\n",
        "        elif self.cfg.model.architecture == 'ResNet18':\n",
        "            model = ResNet18(self.cfg.model.numclasses)\n",
        "        elif self.cfg.model.architecture == 'GoogLeNet':\n",
        "            model = GoogLeNet(self.cfg.model.numclasses)\n",
        "        elif self.cfg.model.architecture == 'MnistNet':\n",
        "            model = MnistNet()\n",
        "        elif self.cfg.model.architecture == 'ResNet164':\n",
        "            model = ResNet164(self.cfg.model.numclasses)\n",
        "        elif self.cfg.model.architecture == 'MobileNet':\n",
        "            model = MobileNet(self.cfg.model.numclasses)\n",
        "        elif self.cfg.model.architecture == 'MobileNetV2':\n",
        "            model = MobileNetV2(self.cfg.model.numclasses)\n",
        "        elif self.cfg.model.architecture == 'MobileNet2':\n",
        "            model = MobileNet2(output_size=self.cfg.model.numclasses)\n",
        "        elif self.cfg.model.architecture == 'HyperParamNet':\n",
        "            model = HyperParamNet(self.cfg.model.l1, self.cfg.model.l2)\n",
        "        elif self.cfg.model.architecture == 'ThreeLayerNet':\n",
        "            model = ThreeLayerNet(self.cfg.model.input_dim, self.cfg.model.numclasses, \n",
        "\t    self.cfg.model.h1, self.cfg.model.h2)\n",
        "        elif self.cfg.model.architecture == 'LSTM':\n",
        "            model = LSTMClassifier(self.cfg.model.numclasses, self.cfg.model.wordvec_dim, \\\n",
        "                 self.cfg.model.weight_path, self.cfg.model.num_layers, self.cfg.model.hidden_size)\n",
        "        else:\n",
        "            raise(NotImplementedError)\n",
        "        model = model.to(self.cfg.train_args.device)\n",
        "        return model\n",
        "\n",
        "    \"\"\"\n",
        "    ############################## Loss Type, Optimizer and Learning Rate Scheduler ##############################\n",
        "    \"\"\"\n",
        "\n",
        "    def loss_function(self):\n",
        "        if self.cfg.loss.type == \"CrossEntropyLoss\":\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            criterion_nored = nn.CrossEntropyLoss(reduction='none')\n",
        "            \n",
        "        elif self.cfg.loss.type == \"MeanSquaredLoss\":\n",
        "            criterion = nn.MSELoss()\n",
        "            criterion_nored = nn.MSELoss(reduction='none')\n",
        "        return criterion, criterion_nored\n",
        "\n",
        "    def optimizer_with_scheduler(self, model):\n",
        "        if self.cfg.optimizer.type == 'sgd':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=self.cfg.optimizer.lr,\n",
        "                                  momentum=self.cfg.optimizer.momentum,\n",
        "                                  weight_decay=self.cfg.optimizer.weight_decay,\n",
        "                                  nesterov=self.cfg.optimizer.nesterov)\n",
        "        elif self.cfg.optimizer.type == \"adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=self.cfg.optimizer.lr)\n",
        "        elif self.cfg.optimizer.type == \"rmsprop\":\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=self.cfg.optimizer.lr)\n",
        "\n",
        "        if self.cfg.scheduler.type == 'cosine_annealing':\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                                   T_max=self.cfg.scheduler.T_max)\n",
        "        elif self.cfg.scheduler.type == 'linear_decay':\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.cfg.scheduler.stepsize, gamma=self.cfg.scheduler.gamma)\n",
        "        else:\n",
        "            scheduler = None\n",
        "        return optimizer, scheduler\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_cumulative_timing(mod_timing):\n",
        "        tmp = 0\n",
        "        mod_cum_timing = np.zeros(len(mod_timing))\n",
        "        for i in range(len(mod_timing)):\n",
        "            tmp += mod_timing[i]\n",
        "            mod_cum_timing[i] = tmp\n",
        "        return mod_cum_timing\n",
        "\n",
        "    @staticmethod\n",
        "    def save_ckpt(state, ckpt_path):\n",
        "        torch.save(state, ckpt_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_ckpt(ckpt_path, model, optimizer):\n",
        "        checkpoint = torch.load(ckpt_path)\n",
        "        start_epoch = checkpoint['epoch']\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        loss = checkpoint['loss']\n",
        "        metrics = checkpoint['metrics']\n",
        "        return start_epoch, model, optimizer, loss, metrics\n",
        "\n",
        "    def count_pkl(self, path):\n",
        "        if not osp.exists(path):\n",
        "            return -1\n",
        "        return_val = 0\n",
        "        file = open(path, 'rb')\n",
        "        while(True):\n",
        "            try:\n",
        "                _ = pickle.load(file)\n",
        "                return_val += 1\n",
        "            except EOFError:\n",
        "                break\n",
        "        file.close()\n",
        "        return return_val\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        ############################## General Training Loop with Data Selection Strategies ##############################\n",
        "        \"\"\"\n",
        "        # Loading the Dataset\n",
        "        logger = self.logger\n",
        "        logger.info(self.cfg)\n",
        "        if self.cfg.dataset.feature == 'classimb':\n",
        "            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,\n",
        "                                                               self.cfg.dataset.name,\n",
        "                                                               self.cfg.dataset.feature,\n",
        "                                                               classimb_ratio=self.cfg.dataset.classimb_ratio, dataset=self.cfg.dataset)\n",
        "        else:\n",
        "            trainset, validset, testset, num_cls = gen_dataset(self.cfg.dataset.datadir,\n",
        "                                                               self.cfg.dataset.name,\n",
        "                                                               self.cfg.dataset.feature, dataset=self.cfg.dataset)\n",
        "\n",
        "        trn_batch_size = self.cfg.dataloader.batch_size\n",
        "        val_batch_size = self.cfg.dataloader.batch_size\n",
        "        tst_batch_size = self.cfg.dataloader.batch_size\n",
        "\n",
        "        if self.cfg.dataset.name == \"sst2_facloc\" and self.count_pkl(self.cfg.dataset.ss_path) == 1 and self.cfg.dss_args.type == 'FacLoc':\n",
        "            self.cfg.dss_args.type = 'Full'\n",
        "            file_ss = open(self.cfg.dataset.ss_path, 'rb')\n",
        "            ss_indices = pickle.load(file_ss)\n",
        "            file_ss.close()\n",
        "            trainset = torch.utils.data.Subset(trainset, ss_indices)\n",
        "\n",
        "        if 'collate_fn' not in self.cfg.dataloader.keys():\n",
        "            collate_fn = None\n",
        "        else:\n",
        "            collate_fn = self.cfg.dataloader.collate_fn\n",
        "\n",
        "        batch_sampler = lambda _, __ : None\n",
        "        drop_last = False\n",
        "        if self.cfg.dss_args.type in ['SELCON']:\n",
        "            drop_last = True\n",
        "            assert(self.cfg.dataset.name in ['LawSchool_selcon', 'Community_Crime'])\n",
        "            if self.cfg.dss_arg.batch_sampler == 'sequential':\n",
        "                batch_sampler = lambda dataset, bs : torch.utils.data.BatchSampler(\n",
        "                    torch.utils.data.SequentialSampler(dataset), batch_size=bs, drop_last=True\n",
        "                )   # sequential\n",
        "            elif self.cfg.dss_arg.batch_sampler == 'random':\n",
        "                batch_sampler = lambda dataset, bs : torch.utils.data.BatchSampler(\n",
        "                    torch.utils.data.RandomSampler(dataset), batch_size=bs, drop_last=True\n",
        "                )   # random\n",
        "\n",
        "\n",
        "        if self.cfg.dataset.name == \"sst2_facloc\" and self.count_pkl(self.cfg.dataset.ss_path) == 1 and self.cfg.dss_args.type == 'FacLoc':\n",
        "            self.cfg.dss_args.type = 'Full'\n",
        "            file_ss = open(self.cfg.dataset.ss_path, 'rb')\n",
        "            ss_indices = pickle.load(file_ss)\n",
        "            file_ss.close()\n",
        "            trainset = torch.utils.data.Subset(trainset, ss_indices)\n",
        "\n",
        "        if 'collate_fn' not in self.cfg.dataloader.keys():\n",
        "            collate_fn = None\n",
        "        else:\n",
        "            collate_fn = self.cfg.dataloader.collate_fn\n",
        "\n",
        "        # Creating the Data Loaders\n",
        "        trainloader = torch.utils.data.DataLoader(trainset, batch_size=trn_batch_size, sampler=batch_sampler(trainset, trn_batch_size),\n",
        "                                                  shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)\n",
        "\n",
        "        valloader = torch.utils.data.DataLoader(validset, batch_size=val_batch_size, sampler=batch_sampler(validset, val_batch_size),\n",
        "                                                shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)\n",
        "\n",
        "        testloader = torch.utils.data.DataLoader(testset, batch_size=tst_batch_size, sampler=batch_sampler(testset, tst_batch_size),\n",
        "                                                 shuffle=False, pin_memory=True, collate_fn = collate_fn, drop_last=drop_last)\n",
        "\n",
        "        substrn_losses = list()  # np.zeros(cfg['train_args']['num_epochs'])\n",
        "        trn_losses = list()\n",
        "        val_losses = list()  # np.zeros(cfg['train_args']['num_epochs'])\n",
        "        tst_losses = list()\n",
        "        subtrn_losses = list()\n",
        "        timing = list()\n",
        "        trn_acc = list()\n",
        "        val_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])\n",
        "        tst_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])\n",
        "        subtrn_acc = list()  # np.zeros(cfg['train_args']['num_epochs'])\n",
        "\n",
        "        # Checkpoint file\n",
        "        checkpoint_dir = osp.abspath(osp.expanduser(self.cfg.ckpt.dir))\n",
        "        ckpt_dir = os.path.join(checkpoint_dir, self.cfg.setting,\n",
        "                                self.cfg.dss_args.type,\n",
        "                                self.cfg.dataset.name,\n",
        "                                str(self.cfg.dss_args.fraction),\n",
        "                                str(self.cfg.dss_args.select_every))\n",
        "        checkpoint_path = os.path.join(ckpt_dir, 'model.pt')\n",
        "        os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "        # Model Creation\n",
        "        model = self.create_model()\n",
        "\n",
        "        # Loss Functions\n",
        "        criterion, criterion_nored = self.loss_function()\n",
        "\n",
        "        # Getting the optimizer and scheduler\n",
        "        optimizer, scheduler = self.optimizer_with_scheduler(model)\n",
        "\n",
        "        \"\"\"\n",
        "        ############################## Custom Dataloader Creation ##############################\n",
        "        \"\"\"\n",
        "\n",
        "        if 'collate_fn' not in self.cfg.dss_args:\n",
        "                self.cfg.dss_args.collate_fn = None\n",
        "\n",
        "        if self.cfg.dss_args.type in ['GradMatch', 'GradMatchPB', 'GradMatch-Warm', 'GradMatchPB-Warm']:\n",
        "            \"\"\"\n",
        "            ############################## GradMatch Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            self.cfg.dss_args.model = model\n",
        "            self.cfg.dss_args.loss = criterion_nored\n",
        "            self.cfg.dss_args.eta = self.cfg.optimizer.lr\n",
        "            self.cfg.dss_args.num_classes = self.cfg.model.numclasses\n",
        "            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs\n",
        "            self.cfg.dss_args.device = self.cfg.train_args.device\n",
        "\n",
        "            dataloader = GradMatchDataLoader(trainloader, valloader, self.cfg.dss_args, logger,\n",
        "                                             batch_size=self.cfg.dataloader.batch_size,\n",
        "                                             shuffle=self.cfg.dataloader.shuffle,\n",
        "                                             pin_memory=self.cfg.dataloader.pin_memory,\n",
        "                                             collate_fn = self.cfg.dss_args.collate_fn)\n",
        "\n",
        "        elif self.cfg.dss_args.type in ['GLISTER', 'GLISTER-Warm', 'GLISTERPB', 'GLISTERPB-Warm']:\n",
        "            \"\"\"\n",
        "            ############################## GLISTER Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            self.cfg.dss_args.model = model\n",
        "            self.cfg.dss_args.loss = criterion_nored\n",
        "            self.cfg.dss_args.eta = self.cfg.optimizer.lr\n",
        "            self.cfg.dss_args.num_classes = self.cfg.model.numclasses\n",
        "            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs\n",
        "            self.cfg.dss_args.device = self.cfg.train_args.device\n",
        "            dataloader = GLISTERDataLoader(trainloader, valloader, self.cfg.dss_args, logger,\n",
        "                                           batch_size=self.cfg.dataloader.batch_size,\n",
        "                                           shuffle=self.cfg.dataloader.shuffle,\n",
        "                                           pin_memory=self.cfg.dataloader.pin_memory,\n",
        "                                           collate_fn = self.cfg.dss_args.collate_fn)\n",
        "\n",
        "        elif self.cfg.dss_args.type in ['CRAIG', 'CRAIG-Warm', 'CRAIGPB', 'CRAIGPB-Warm']:\n",
        "            \"\"\"\n",
        "            ############################## CRAIG Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            self.cfg.dss_args.model = model\n",
        "            self.cfg.dss_args.loss = criterion_nored\n",
        "            self.cfg.dss_args.num_classes = self.cfg.model.numclasses\n",
        "            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs\n",
        "            self.cfg.dss_args.device = self.cfg.train_args.device\n",
        "\n",
        "            dataloader = CRAIGDataLoader(trainloader, valloader, self.cfg.dss_args, logger,\n",
        "                                         batch_size=self.cfg.dataloader.batch_size,\n",
        "                                         shuffle=self.cfg.dataloader.shuffle,\n",
        "                                         pin_memory=self.cfg.dataloader.pin_memory,\n",
        "                                         collate_fn = self.cfg.dss_args.collate_fn)\n",
        "\n",
        "        elif self.cfg.dss_args.type in ['Random', 'Random-Warm']:\n",
        "            \"\"\"\n",
        "            ############################## Random Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            self.cfg.dss_args.device = self.cfg.train_args.device\n",
        "            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs\n",
        "\n",
        "            dataloader = RandomDataLoader(trainloader, self.cfg.dss_args, logger,\n",
        "                                          batch_size=self.cfg.dataloader.batch_size,\n",
        "                                          shuffle=self.cfg.dataloader.shuffle,\n",
        "                                          pin_memory=self.cfg.dataloader.pin_memory, \n",
        "                                          collate_fn = self.cfg.dss_args.collate_fn)\n",
        "\n",
        "        elif self.cfg.dss_args.type == ['OLRandom', 'OLRandom-Warm']:\n",
        "            \"\"\"\n",
        "            ############################## OLRandom Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            self.cfg.dss_args.device = self.cfg.train_args.device\n",
        "            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs\n",
        "\n",
        "            dataloader = OLRandomDataLoader(trainloader, self.cfg.dss_args, logger,\n",
        "                                            batch_size=self.cfg.dataloader.batch_size,\n",
        "                                            shuffle=self.cfg.dataloader.shuffle,\n",
        "                                            pin_memory=self.cfg.dataloader.pin_memory,\n",
        "                                            collate_fn = self.cfg.dss_args.collate_fn)\n",
        "\n",
        "        elif self.cfg.dss_args.type == 'FacLoc':\n",
        "            \"\"\"\n",
        "            ############################## Facility Location Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))\n",
        "            self.cfg.dss_args.device = self.cfg.train_args.device\n",
        "            self.cfg.dss_args.model = model\n",
        "            self.cfg.dss_args.data_type = self.cfg.dataset.type\n",
        "            \n",
        "            dataloader = FacLocDataLoader(trainloader, valloader, self.cfg.dss_args, logger, \n",
        "                                          batch_size=self.cfg.dataloader.batch_size,\n",
        "                                          shuffle=self.cfg.dataloader.shuffle,\n",
        "                                          pin_memory=self.cfg.dataloader.pin_memory, \n",
        "                                          collate_fn = self.cfg.dss_args.collate_fn)\n",
        "            if self.cfg.dataset.name == \"sst2_facloc\" and self.count_pkl(self.cfg.dataset.ss_path) < 1:\n",
        "\n",
        "                ss_indices = dataloader.subset_indices\n",
        "                file_ss = open(self.cfg.dataset.ss_path, 'wb')\n",
        "                try:\n",
        "                    pickle.dump(ss_indices, file_ss)\n",
        "                except EOFError:\n",
        "                    pass\n",
        "                file_ss.close()\n",
        "\n",
        "        elif self.cfg.dss_args.type == 'Full':\n",
        "            \"\"\"\n",
        "            ############################## Full Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            wt_trainset = WeightedSubset(trainset, list(range(len(trainset))), [1] * len(trainset))\n",
        "\n",
        "            dataloader = torch.utils.data.DataLoader(wt_trainset,\n",
        "                                                     batch_size=self.cfg.dataloader.batch_size,\n",
        "                                                     shuffle=self.cfg.dataloader.shuffle,\n",
        "                                                     pin_memory=self.cfg.dataloader.pin_memory,\n",
        "                                                     collate_fn=self.cfg.dss_args.collate_fn)\n",
        "\n",
        "        elif self.cfg.dss_args.type in ['SELCON']:\n",
        "            \"\"\"\n",
        "            ############################## SELCON Dataloader Additional Arguments ##############################\n",
        "            \"\"\"\n",
        "            self.cfg.dss_args.model = model\n",
        "            self.cfg.dss_args.lr = self.cfg.optimizer.lr\n",
        "            self.cfg.dss_args.loss = criterion_nored # doubt: or criterion\n",
        "            self.cfg.dss_args.device = self.cfg.train_args.device\n",
        "            self.cfg.dss_args.optimizer = optimizer\n",
        "            self.cfg.dss_args.criterion = criterion\n",
        "            self.cfg.dss_args.num_classes = self.cfg.model.numclasses\n",
        "            self.cfg.dss_args.batch_size = self.cfg.dataloader.batch_size\n",
        "            \n",
        "            # todo: not done yet\n",
        "            self.cfg.dss_args.delta = torch.tensor(self.cfg.dss_args.delta)\n",
        "            # self.cfg.dss_args.linear_layer = self.cfg.dss_args.linear_layer # already there, check glister init\n",
        "            self.cfg.dss_args.num_epochs = self.cfg.train_args.num_epochs\n",
        "            \n",
        "            dataloader = SELCONDataLoader(trainset, validset, trainloader, valloader, self.cfg.dss_args, logger,\n",
        "                                           batch_size=self.cfg.dataloader.batch_size,\n",
        "                                           shuffle=self.cfg.dataloader.shuffle,\n",
        "                                           pin_memory=self.cfg.dataloader.pin_memory)\n",
        "\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        if self.cfg.dss_args.type in ['SELCON']:        \n",
        "            is_selcon = True\n",
        "        else:\n",
        "            is_selcon = False\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        ################################################# Checkpoint Loading #################################################\n",
        "        \"\"\"\n",
        "\n",
        "        if self.cfg.ckpt.is_load:\n",
        "            start_epoch, model, optimizer, ckpt_loss, load_metrics = self.load_ckpt(checkpoint_path, model, optimizer)\n",
        "            logger.info(\"Loading saved checkpoint model at epoch: {0:d}\".format(start_epoch))\n",
        "            for arg in load_metrics.keys():\n",
        "                if arg == \"val_loss\":\n",
        "                    val_losses = load_metrics['val_loss']\n",
        "                if arg == \"val_acc\":\n",
        "                    val_acc = load_metrics['val_acc']\n",
        "                if arg == \"tst_loss\":\n",
        "                    tst_losses = load_metrics['tst_loss']\n",
        "                if arg == \"tst_acc\":\n",
        "                    tst_acc = load_metrics['tst_acc']\n",
        "                if arg == \"trn_loss\":\n",
        "                    trn_losses = load_metrics['trn_loss']\n",
        "                if arg == \"trn_acc\":\n",
        "                    trn_acc = load_metrics['trn_acc']\n",
        "                if arg == \"subtrn_loss\":\n",
        "                    subtrn_losses = load_metrics['subtrn_loss']\n",
        "                if arg == \"subtrn_acc\":\n",
        "                    subtrn_acc = load_metrics['subtrn_acc']\n",
        "                if arg == \"time\":\n",
        "                    timing = load_metrics['time']\n",
        "        else:\n",
        "            start_epoch = 0\n",
        "\n",
        "        \"\"\"\n",
        "        ################################################# Training Loop #################################################\n",
        "        \"\"\"\n",
        "\n",
        "        for epoch in range(start_epoch, self.cfg.train_args.num_epochs):\n",
        "            subtrn_loss = 0\n",
        "            subtrn_correct = 0\n",
        "            subtrn_total = 0\n",
        "            model.train()\n",
        "            start_time = time.time()\n",
        "            cum_weights = 0\n",
        "            for _, data in enumerate(dataloader):\n",
        "                if is_selcon:\n",
        "                    inputs, targets, _, weights = data  # dataloader also returns id in case of selcon algorithm\n",
        "                else:\n",
        "                    inputs, targets, weights = data\n",
        "                inputs = inputs.to(self.cfg.train_args.device)\n",
        "                targets = targets.to(self.cfg.train_args.device, non_blocking=True)\n",
        "                weights = weights.to(self.cfg.train_args.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                losses = criterion_nored(outputs, targets)\n",
        "                if self.cfg.is_reg:\n",
        "                    loss = torch.dot(losses.view(-1), weights / (weights.sum()))\n",
        "                else:\n",
        "                    loss = torch.dot(losses, weights / (weights.sum()))\n",
        "                loss.backward()\n",
        "                subtrn_loss += (loss.item() * weights.sum())\n",
        "                cum_weights += weights.sum()\n",
        "                optimizer.step()\n",
        "                if not self.cfg.is_reg:\n",
        "                    if is_selcon:\n",
        "                        predicted = outputs     # linaer regression in selcon\n",
        "                    else:\n",
        "                        _, predicted = outputs.max(1)\n",
        "                    subtrn_total += targets.size(0)\n",
        "                    subtrn_correct += predicted.eq(targets).sum().item()\n",
        "            epoch_time = time.time() - start_time\n",
        "            if cum_weights != 0:\n",
        "                subtrn_loss = subtrn_loss/cum_weights\n",
        "            if not scheduler == None:\n",
        "                scheduler.step()\n",
        "            timing.append(epoch_time)\n",
        "            print_args = self.cfg.train_args.print_args\n",
        "\n",
        "            \"\"\"\n",
        "            ################################################# Evaluation Loop #################################################\n",
        "            \"\"\"\n",
        "\n",
        "            if ((epoch + 1) % self.cfg.train_args.print_every == 0) or (epoch == self.cfg.train_args.num_epochs - 1):\n",
        "                trn_loss = 0\n",
        "                trn_correct = 0\n",
        "                trn_total = 0\n",
        "                val_loss = 0\n",
        "                val_correct = 0\n",
        "                val_total = 0\n",
        "                tst_correct = 0\n",
        "                tst_total = 0\n",
        "                tst_loss = 0\n",
        "                model.eval()\n",
        "\n",
        "                if (\"trn_loss\" in print_args) or (\"trn_acc\" in print_args):\n",
        "                    samples =0\n",
        "                    with torch.no_grad():\n",
        "                        for _, data in enumerate(trainloader):\n",
        "                            if is_selcon:\n",
        "                                inputs, targets, _ = data\n",
        "                            else:\n",
        "                                inputs, targets = data\n",
        "\n",
        "                            inputs, targets = inputs.to(self.cfg.train_args.device), \\\n",
        "                                              targets.to(self.cfg.train_args.device, non_blocking=True)\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, targets)\n",
        "                            trn_loss += (loss.item() * trainloader.batch_size)\n",
        "                            samples += targets.shape[0]\n",
        "                            if \"trn_acc\" in print_args:\n",
        "                                if is_selcon: predicted = outputs\n",
        "                                else: _, predicted = outputs.max(1)\n",
        "                                trn_total += targets.size(0)\n",
        "                                trn_correct += predicted.eq(targets).sum().item()\n",
        "                        trn_loss = trn_loss/samples\n",
        "                        trn_losses.append(trn_loss)\n",
        "\n",
        "                    if \"trn_acc\" in print_args:\n",
        "                        trn_acc.append(trn_correct / trn_total)\n",
        "\n",
        "                if (\"val_loss\" in print_args) or (\"val_acc\" in print_args):\n",
        "                    samples =0\n",
        "                    with torch.no_grad():\n",
        "                        for _, data in enumerate(valloader):\n",
        "                            if is_selcon:\n",
        "                                inputs, targets, _ = data\n",
        "                            else:\n",
        "                                inputs, targets = data\n",
        "\n",
        "                            inputs, targets = inputs.to(self.cfg.train_args.device), \\\n",
        "                                              targets.to(self.cfg.train_args.device, non_blocking=True)\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, targets)\n",
        "                            val_loss += (loss.item() * valloader.batch_size)\n",
        "                            samples += targets.shape[0]\n",
        "                            if \"val_acc\" in print_args:\n",
        "                                if is_selcon: predicted = outputs\n",
        "                                else: _, predicted = outputs.max(1)\n",
        "                                val_total += targets.size(0)\n",
        "                                val_correct += predicted.eq(targets).sum().item()\n",
        "                        val_loss = val_loss/samples\n",
        "                        val_losses.append(val_loss)\n",
        "\n",
        "                    if \"val_acc\" in print_args:\n",
        "                        val_acc.append(val_correct / val_total)\n",
        "\n",
        "                if (\"tst_loss\" in print_args) or (\"tst_acc\" in print_args):\n",
        "                    samples =0\n",
        "                    with torch.no_grad():\n",
        "                        for _, data in enumerate(testloader):\n",
        "                            if is_selcon:\n",
        "                                inputs, targets, _ = data\n",
        "                            else:\n",
        "                                inputs, targets = data\n",
        "\n",
        "                            inputs, targets = inputs.to(self.cfg.train_args.device), \\\n",
        "                                              targets.to(self.cfg.train_args.device, non_blocking=True)\n",
        "                            outputs = model(inputs)\n",
        "                            loss = criterion(outputs, targets)\n",
        "                            tst_loss += (loss.item() * testloader.batch_size)\n",
        "                            samples += targets.shape[0]\n",
        "                            if \"tst_acc\" in print_args:\n",
        "                                if is_selcon: predicted = outputs\n",
        "                                else: _, predicted = outputs.max(1)\n",
        "                                tst_total += targets.size(0)\n",
        "                                tst_correct += predicted.eq(targets).sum().item()\n",
        "                        tst_loss = tst_loss/samples\n",
        "                        tst_losses.append(tst_loss)\n",
        "\n",
        "                    if \"tst_acc\" in print_args:\n",
        "                        tst_acc.append(tst_correct / tst_total)\n",
        "\n",
        "                if \"subtrn_acc\" in print_args:\n",
        "                    subtrn_acc.append(subtrn_correct / subtrn_total)\n",
        "\n",
        "                if \"subtrn_losses\" in print_args:\n",
        "                    subtrn_losses.append(subtrn_loss)\n",
        "\n",
        "                print_str = \"Epoch: \" + str(epoch + 1)\n",
        "\n",
        "                \"\"\"\n",
        "                ################################################# Results Printing #################################################\n",
        "                \"\"\"\n",
        "\n",
        "                for arg in print_args:\n",
        "\n",
        "                    if arg == \"val_loss\":\n",
        "                        print_str += \" , \" + \"Validation Loss: \" + str(val_losses[-1])\n",
        "\n",
        "                    if arg == \"val_acc\":\n",
        "                        print_str += \" , \" + \"Validation Accuracy: \" + str(val_acc[-1])\n",
        "\n",
        "                    if arg == \"tst_loss\":\n",
        "                        print_str += \" , \" + \"Test Loss: \" + str(tst_losses[-1])\n",
        "\n",
        "                    if arg == \"tst_acc\":\n",
        "                        print_str += \" , \" + \"Test Accuracy: \" + str(tst_acc[-1])\n",
        "\n",
        "                    if arg == \"trn_loss\":\n",
        "                        print_str += \" , \" + \"Training Loss: \" + str(trn_losses[-1])\n",
        "\n",
        "                    if arg == \"trn_acc\":\n",
        "                        print_str += \" , \" + \"Training Accuracy: \" + str(trn_acc[-1])\n",
        "\n",
        "                    if arg == \"subtrn_loss\":\n",
        "                        print_str += \" , \" + \"Subset Loss: \" + str(subtrn_losses[-1])\n",
        "\n",
        "                    if arg == \"subtrn_acc\":\n",
        "                        print_str += \" , \" + \"Subset Accuracy: \" + str(subtrn_acc[-1])\n",
        "\n",
        "                    if arg == \"time\":\n",
        "                        print_str += \" , \" + \"Timing: \" + str(timing[-1])\n",
        "\n",
        "                # report metric to ray for hyperparameter optimization\n",
        "                if 'report_tune' in self.cfg and self.cfg.report_tune  and len(dataloader):\n",
        "                    tune.report(mean_accuracy=val_acc[-1])\n",
        "\n",
        "                logger.info(print_str)\n",
        "\n",
        "            \"\"\"\n",
        "            ################################################# Checkpoint Saving #################################################\n",
        "            \"\"\"\n",
        "\n",
        "            if ((epoch + 1) % self.cfg.ckpt.save_every == 0) and self.cfg.ckpt.is_save:\n",
        "\n",
        "                metric_dict = {}\n",
        "\n",
        "                for arg in print_args:\n",
        "                    if arg == \"val_loss\":\n",
        "                        metric_dict['val_loss'] = val_losses\n",
        "                    if arg == \"val_acc\":\n",
        "                        metric_dict['val_acc'] = val_acc\n",
        "                    if arg == \"tst_loss\":\n",
        "                        metric_dict['tst_loss'] = tst_losses\n",
        "                    if arg == \"tst_acc\":\n",
        "                        metric_dict['tst_acc'] = tst_acc\n",
        "                    if arg == \"trn_loss\":\n",
        "                        metric_dict['trn_loss'] = trn_losses\n",
        "                    if arg == \"trn_acc\":\n",
        "                        metric_dict['trn_acc'] = trn_acc\n",
        "                    if arg == \"subtrn_loss\":\n",
        "                        metric_dict['subtrn_loss'] = subtrn_losses\n",
        "                    if arg == \"subtrn_acc\":\n",
        "                        metric_dict['subtrn_acc'] = subtrn_acc\n",
        "                    if arg == \"time\":\n",
        "                        metric_dict['time'] = timing\n",
        "\n",
        "                ckpt_state = {\n",
        "                    'epoch': epoch + 1,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'loss': self.loss_function(),\n",
        "                    'metrics': metric_dict\n",
        "                }\n",
        "\n",
        "                # save checkpoint\n",
        "                self.save_ckpt(ckpt_state, checkpoint_path)\n",
        "                logger.info(\"Model checkpoint saved at epoch: {0:d}\".format(epoch + 1))\n",
        "\n",
        "        \"\"\"\n",
        "        ################################################# Results Summary #################################################\n",
        "        \"\"\"\n",
        "\n",
        "        logger.info(self.cfg.dss_args.type + \" Selection Run---------------------------------\")\n",
        "        logger.info(\"Final SubsetTrn: {0:f}\".format(subtrn_loss))\n",
        "        if \"val_loss\" in print_args:\n",
        "            if \"val_acc\" in print_args:\n",
        "                logger.info(\"Validation Loss: %.2f , Validation Accuracy: %.2f\", val_loss, val_acc[-1])\n",
        "            else:\n",
        "                logger.info(\"Validation Loss: %.2f\", val_loss)\n",
        "\n",
        "        if \"tst_loss\" in print_args:\n",
        "            if \"tst_acc\" in print_args:\n",
        "                logger.info(\"Test Loss: %.2f, Test Accuracy: %.2f\", tst_loss, tst_acc[-1])\n",
        "            else:\n",
        "                logger.info(\"Test Data Loss: %f\", tst_loss)\n",
        "        logger.info('---------------------------------------------------------------------')\n",
        "        logger.info(self.cfg.dss_args.type)\n",
        "        logger.info('---------------------------------------------------------------------')\n",
        "\n",
        "        \"\"\"\n",
        "        ################################################# Final Results Logging #################################################\n",
        "        \"\"\"\n",
        "\n",
        "        if \"val_acc\" in print_args:\n",
        "            val_str = \"Validation Accuracy: \"\n",
        "            for val in val_acc:\n",
        "                if val_str == \"Validation Accuracy: \":\n",
        "                    val_str = val_str + str(val)\n",
        "                else:\n",
        "                    val_str = val_str + \" , \" + str(val)\n",
        "            logger.info(val_str)\n",
        "\n",
        "        if \"tst_acc\" in print_args:\n",
        "            tst_str = \"Test Accuracy: \"\n",
        "            for tst in tst_acc:\n",
        "                if tst_str == \"Test Accuracy: \":\n",
        "                    tst_str = tst_str + str(tst)\n",
        "                else:\n",
        "                    tst_str = tst_str + \" , \" + str(tst)\n",
        "            logger.info(tst_str)\n",
        "\n",
        "        if \"time\" in print_args:\n",
        "            time_str = \"Time: \"\n",
        "            for t in timing:\n",
        "                if time_str == \"Time: \":\n",
        "                    time_str = time_str + str(t)\n",
        "                else:\n",
        "                    time_str = time_str + \" , \" + str(t)\n",
        "            logger.info(time_str)\n",
        "\n",
        "        omp_timing = np.array(timing)\n",
        "        omp_cum_timing = list(self.generate_cumulative_timing(omp_timing))\n",
        "        logger.info(\"Total time taken by %s = %.4f \", self.cfg.dss_args.type, omp_cum_timing[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJZgHJMCioR7"
      },
      "outputs": [],
      "source": [
        "full_config_file = \"/content/cords/configs/SL/config_full_cifar100.py\"\n",
        "dataset_name = \"cifar100\"\n",
        "cfg = load_config_data(full_config_file) \n",
        "full_trn = TrainClassifier(cfg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xdr-dx5i8Rm"
      },
      "outputs": [],
      "source": [
        "full_trn.cfg.model.architecture = 'GoogLeNet'\n",
        "full_trn.cfg.train_args.num_epochs = 100\n",
        "full_trn.cfg.train_args.print_every = 100\n",
        "full_trn.cfg.train.tst_batch_size = 1200\n",
        "full_trn.cfg.optimizer.lr = 0.1 \n",
        "full_trn.cfg.train_args.results_dir = \"/content/drive/MyDrive/results/\"\n",
        "full_trn.cfg.model.numclasses = numclasses\n",
        "full_trn.cfg.dataset.name = dataset_name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "full_trn.train() # 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQCAfxNAN6nd",
        "outputId": "9c31b078-d078-4d26-dc81-2bef91103ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11/17 06:20:10] __main__ INFO: DotMap(setting='SL', is_reg=False, dataset=DotMap(name='cifar100', datadir='../data', feature='dss', type='image'), dataloader=DotMap(shuffle=True, batch_size=20, pin_memory=True), model=DotMap(architecture='GoogLeNet', type='pre-defined', numclasses=100), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='CrossEntropyLoss', use_sigmoid=False), optimizer=DotMap(type='sgd', momentum=0.9, lr=0.1, lr1=0.01, lr2=0.01, lr3=0.01, nesterov=False, weight_decay=0.0005), scheduler=DotMap(type='cosine_annealing', T_max=300), dss_args=DotMap(type='Full', fraction=DotMap(), select_every=DotMap(), collate_fn=None), train_args=DotMap(num_epochs=100, device='cuda', print_every=100, results_dir='/content/drive/MyDrive/results/', print_args=['val_loss', 'val_acc', 'tst_loss', 'tst_acc', 'time'], return_args=[]), train=DotMap(tst_batch_size=1200))\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[11/17 07:03:11] __main__ INFO: Model checkpoint saved at epoch: 20\n",
            "[11/17 07:46:09] __main__ INFO: Model checkpoint saved at epoch: 40\n",
            "[11/17 08:29:03] __main__ INFO: Model checkpoint saved at epoch: 60\n",
            "[11/17 09:11:54] __main__ INFO: Model checkpoint saved at epoch: 80\n",
            "[11/17 09:55:12] __main__ INFO: Epoch: 100 , Validation Loss: 2.2934547901153564 , Validation Accuracy: 0.4046 , Test Loss: 2.2800741157531736 , Test Accuracy: 0.4072 , Timing: 129.90008974075317\n",
            "[11/17 09:55:12] __main__ INFO: Model checkpoint saved at epoch: 100\n",
            "[11/17 09:55:12] __main__ INFO: Full Selection Run---------------------------------\n",
            "[11/17 09:55:12] __main__ INFO: Final SubsetTrn: 2.185281\n",
            "[11/17 09:55:12] __main__ INFO: Validation Loss: 2.29 , Validation Accuracy: 0.40\n",
            "[11/17 09:55:12] __main__ INFO: Test Loss: 2.28, Test Accuracy: 0.41\n",
            "[11/17 09:55:12] __main__ INFO: ---------------------------------------------------------------------\n",
            "[11/17 09:55:12] __main__ INFO: Full\n",
            "[11/17 09:55:12] __main__ INFO: ---------------------------------------------------------------------\n",
            "[11/17 09:55:12] __main__ INFO: Validation Accuracy: 0.4046\n",
            "[11/17 09:55:12] __main__ INFO: Test Accuracy: 0.4072\n",
            "[11/17 09:55:12] __main__ INFO: Time: 129.2063066959381 , 129.64107584953308 , 128.92605352401733 , 129.5822241306305 , 128.59211134910583 , 129.00949954986572 , 128.4619677066803 , 128.8214626312256 , 128.99628138542175 , 129.2257800102234 , 129.3170759677887 , 129.02528166770935 , 129.20222973823547 , 128.2977831363678 , 128.6425633430481 , 128.5653522014618 , 129.0804147720337 , 128.8994176387787 , 128.89965271949768 , 128.86228680610657 , 129.11236596107483 , 128.32074761390686 , 128.9608154296875 , 128.85570120811462 , 129.12546038627625 , 129.55648183822632 , 129.22417521476746 , 128.71127152442932 , 129.1826343536377 , 128.98502349853516 , 128.46698641777039 , 129.05508399009705 , 128.821542263031 , 128.61754751205444 , 128.3870141506195 , 128.98551201820374 , 128.86542987823486 , 128.4428834915161 , 128.75290417671204 , 128.9886932373047 , 128.3570737838745 , 128.0031361579895 , 129.17326617240906 , 128.00157642364502 , 128.62090587615967 , 128.66485238075256 , 128.8423421382904 , 128.39474248886108 , 129.02911686897278 , 128.41490983963013 , 129.10861134529114 , 128.71353363990784 , 128.206072807312 , 129.6819818019867 , 128.78784728050232 , 128.47515773773193 , 129.43508863449097 , 128.94927191734314 , 128.6263656616211 , 128.98604655265808 , 128.56136107444763 , 128.04734134674072 , 128.22296714782715 , 128.2555615901947 , 128.27556490898132 , 128.15876269340515 , 128.68920397758484 , 128.00690984725952 , 128.96092677116394 , 128.80422115325928 , 128.44505429267883 , 128.76018142700195 , 128.36155104637146 , 128.66091513633728 , 128.6128327846527 , 128.93134927749634 , 129.16584300994873 , 128.6213538646698 , 128.70686078071594 , 128.63503766059875 , 128.35524249076843 , 129.19821977615356 , 128.86440348625183 , 128.48718237876892 , 128.90584444999695 , 129.1867618560791 , 130.07292938232422 , 129.1106276512146 , 129.23693180084229 , 129.01139044761658 , 129.55098247528076 , 128.67461276054382 , 129.77870845794678 , 129.00600266456604 , 129.67350316047668 , 129.32057976722717 , 128.44619059562683 , 129.699862241745 , 128.47470021247864 , 129.90008974075317\n",
            "[11/17 09:55:12] __main__ INFO: Total time taken by Full = 12884.9836 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "67ff9602587d48edb663f63e15bf2acb",
            "30286934e3804eaf8385da2778480574",
            "fc4d0f4f85f2459abc8c4277f5b4cfc7",
            "ae5ee25e830e471aa129b9fafe3d639d",
            "ea4fad0b9bb248f1ba69dabe51d5cb3f",
            "32adbc9fc9914849b6ede0e9cc1162f1",
            "4b486c2e1545458abd9208a33bebf604",
            "6a07dcc199d14bdd925f47f0810b1943",
            "f9ea0150c1294c0c9de5f3658cd45c53",
            "615c9f00c3734c1cad5bc226899539a9",
            "b68a637bf86941a7bdfc8dcd64cc4cab"
          ]
        },
        "id": "dlq1YsityRVm",
        "outputId": "82e20127-b42b-40de-9777-468164a44360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11/17 02:43:15] __main__ INFO: DotMap(setting='SL', is_reg=False, dataset=DotMap(name='cifar100', datadir='../data', feature='dss', type='image'), dataloader=DotMap(shuffle=True, batch_size=20, pin_memory=True), model=DotMap(architecture='GoogLeNet', type='pre-defined', numclasses=100), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='CrossEntropyLoss', use_sigmoid=False), optimizer=DotMap(type='sgd', momentum=0.9, lr=0.01, lr1=0.01, lr2=0.01, lr3=0.01, nesterov=False, weight_decay=0.0005), scheduler=DotMap(type='cosine_annealing', T_max=300), dss_args=DotMap(type='Full'), train_args=DotMap(num_epochs=100, device='cuda', print_every=100, results_dir='/content/drive/MyDrive/results/', print_args=['val_loss', 'val_acc', 'tst_loss', 'tst_acc', 'time'], return_args=[]), train=DotMap(tst_batch_size=1200))\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/169001437 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67ff9602587d48edb663f63e15bf2acb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/cifar-100-python.tar.gz to ../data\n",
            "Files already downloaded and verified\n",
            "[11/17 03:26:46] __main__ INFO: Model checkpoint saved at epoch: 20\n",
            "[11/17 04:10:15] __main__ INFO: Model checkpoint saved at epoch: 40\n",
            "[11/17 04:53:35] __main__ INFO: Model checkpoint saved at epoch: 60\n",
            "[11/17 05:36:52] __main__ INFO: Model checkpoint saved at epoch: 80\n",
            "[11/17 06:20:09] __main__ INFO: Epoch: 100 , Validation Loss: 1.1510555002093315 , Validation Accuracy: 0.7016 , Test Loss: 1.1893242469131946 , Test Accuracy: 0.698 , Timing: 128.3191590309143\n",
            "[11/17 06:20:09] __main__ INFO: Model checkpoint saved at epoch: 100\n",
            "[11/17 06:20:09] __main__ INFO: Full Selection Run---------------------------------\n",
            "[11/17 06:20:09] __main__ INFO: Final SubsetTrn: 0.461088\n",
            "[11/17 06:20:09] __main__ INFO: Validation Loss: 1.15 , Validation Accuracy: 0.70\n",
            "[11/17 06:20:09] __main__ INFO: Test Loss: 1.19, Test Accuracy: 0.70\n",
            "[11/17 06:20:09] __main__ INFO: ---------------------------------------------------------------------\n",
            "[11/17 06:20:09] __main__ INFO: Full\n",
            "[11/17 06:20:09] __main__ INFO: ---------------------------------------------------------------------\n",
            "[11/17 06:20:09] __main__ INFO: Validation Accuracy: 0.7016\n",
            "[11/17 06:20:09] __main__ INFO: Test Accuracy: 0.698\n",
            "[11/17 06:20:09] __main__ INFO: Time: 136.8499298095703 , 130.04421997070312 , 129.6767828464508 , 129.83029770851135 , 129.58268666267395 , 129.76773476600647 , 130.0207269191742 , 129.8793659210205 , 129.86348605155945 , 129.37354230880737 , 129.0740842819214 , 129.83667874336243 , 128.99401664733887 , 129.24703812599182 , 129.68914365768433 , 129.14649152755737 , 129.34600234031677 , 129.92934370040894 , 129.4311649799347 , 129.1395411491394 , 129.5908477306366 , 130.09792494773865 , 129.87431740760803 , 132.1714985370636 , 130.9089560508728 , 131.00045561790466 , 130.78791522979736 , 132.24167728424072 , 130.40207958221436 , 129.955148935318 , 130.38744497299194 , 129.45428037643433 , 130.00424933433533 , 131.8745174407959 , 129.63554215431213 , 130.0567123889923 , 130.27419638633728 , 130.20838594436646 , 129.81902742385864 , 129.90878462791443 , 129.79248046875 , 129.60365414619446 , 130.563946723938 , 129.61948227882385 , 129.8420808315277 , 130.33453941345215 , 129.62812638282776 , 130.00827527046204 , 130.13038229942322 , 130.5685908794403 , 129.68905377388 , 129.70029044151306 , 130.3088891506195 , 129.90972137451172 , 130.04778218269348 , 129.8757357597351 , 130.17780923843384 , 129.51133108139038 , 130.42492699623108 , 129.62235808372498 , 130.24225854873657 , 129.81992721557617 , 130.48907995224 , 129.8966419696808 , 129.3360412120819 , 130.46719002723694 , 129.78919506072998 , 130.3429250717163 , 129.51371455192566 , 129.93853735923767 , 129.93874430656433 , 130.28046226501465 , 130.18312215805054 , 129.6266052722931 , 129.29822087287903 , 129.98258137702942 , 129.88784003257751 , 129.09667110443115 , 129.24041056632996 , 129.540372133255 , 129.0937476158142 , 129.28276777267456 , 129.04755115509033 , 128.9726448059082 , 129.28242564201355 , 129.8328983783722 , 128.96274852752686 , 129.25926160812378 , 129.57071113586426 , 128.66663765907288 , 129.8245313167572 , 128.7678735256195 , 129.12910509109497 , 129.03979921340942 , 128.95480585098267 , 128.71231698989868 , 130.22173023223877 , 128.51439785957336 , 129.2690737247467 , 128.3191590309143\n",
            "[11/17 06:20:09] __main__ INFO: Total time taken by Full = 12986.3704 \n"
          ]
        }
      ],
      "source": [
        "full_trn.train() # epochs = 20 lr = 0.01"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67ff9602587d48edb663f63e15bf2acb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30286934e3804eaf8385da2778480574",
              "IPY_MODEL_fc4d0f4f85f2459abc8c4277f5b4cfc7",
              "IPY_MODEL_ae5ee25e830e471aa129b9fafe3d639d"
            ],
            "layout": "IPY_MODEL_ea4fad0b9bb248f1ba69dabe51d5cb3f"
          }
        },
        "30286934e3804eaf8385da2778480574": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32adbc9fc9914849b6ede0e9cc1162f1",
            "placeholder": "​",
            "style": "IPY_MODEL_4b486c2e1545458abd9208a33bebf604",
            "value": "100%"
          }
        },
        "fc4d0f4f85f2459abc8c4277f5b4cfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a07dcc199d14bdd925f47f0810b1943",
            "max": 169001437,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9ea0150c1294c0c9de5f3658cd45c53",
            "value": 169001437
          }
        },
        "ae5ee25e830e471aa129b9fafe3d639d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_615c9f00c3734c1cad5bc226899539a9",
            "placeholder": "​",
            "style": "IPY_MODEL_b68a637bf86941a7bdfc8dcd64cc4cab",
            "value": " 169001437/169001437 [00:03&lt;00:00, 48682050.53it/s]"
          }
        },
        "ea4fad0b9bb248f1ba69dabe51d5cb3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32adbc9fc9914849b6ede0e9cc1162f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b486c2e1545458abd9208a33bebf604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a07dcc199d14bdd925f47f0810b1943": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ea0150c1294c0c9de5f3658cd45c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "615c9f00c3734c1cad5bc226899539a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68a637bf86941a7bdfc8dcd64cc4cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
